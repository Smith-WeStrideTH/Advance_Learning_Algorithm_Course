{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f2c8a0f5",
   "metadata": {},
   "source": [
    "# ห้องปฏิบัติการเสริม - ฟังก์ชันการกระตุ้น ReLU "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ee2f634",
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.colab import output\n",
    "output.enable_custom_widget_manager()\n",
    "try:\n",
    "  %matplotlib widget\n",
    "  print(\"widget is already installed\")\n",
    "except:\n",
    "  print(\"widget is not been installed, install now..\")\n",
    "  !pip install ipympl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0b74665",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import requests\n",
    "from pathlib import Path\n",
    "\n",
    "url = 'https://raw.githubusercontent.com/Smith-WeStrideTH/Advance_Learning_Algorithm_Course/refs/heads/main/work/deeplearning.mplstyle'\n",
    "url2 = 'https://raw.githubusercontent.com/Smith-WeStrideTH/Advance_Learning_Algorithm_Course/refs/heads/main/work/lab_utils_common_C5.py'\n",
    "url3 = 'https://raw.githubusercontent.com/Smith-WeStrideTH/Advance_Learning_Algorithm_Course/refs/heads/main/work/autils.py'\n",
    "url4 = 'https://raw.githubusercontent.com/Smith-WeStrideTH/Advance_Learning_Algorithm_Course/refs/heads/main/work/lab_utils_relu.py'\n",
    "\n",
    "response = requests.get(url)\n",
    "with open('deeplearning.mplstyle', 'wb') as f:\n",
    "  f.write(response.content)\n",
    "\n",
    "response = requests.get(url2)\n",
    "with open('lab_utils_common.py', 'wb') as f:\n",
    "  f.write(response.content)\n",
    "\n",
    "response = requests.get(url3)\n",
    "with open('autils.py', 'wb') as f:\n",
    "  f.write(response.content)\n",
    "\n",
    "response = requests.get(url4)\n",
    "with open('lab_utils_relu.py', 'wb') as f:\n",
    "  f.write(response.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5b944eec",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.gridspec import GridSpec\n",
    "plt.style.use('./deeplearning.mplstyle')\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, LeakyReLU\n",
    "from tensorflow.keras.activations import linear, relu, sigmoid\n",
    "%matplotlib widget\n",
    "from matplotlib.widgets import Slider\n",
    "from lab_utils_common import dlc\n",
    "from autils import plt_act_trio\n",
    "from lab_utils_relu import *\n",
    "import warnings\n",
    "warnings.simplefilter(action='ignore', category=UserWarning)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f56cdec",
   "metadata": {},
   "source": [
    "<a name=\"2\"></a>\n",
    "## 2 - ReLU Activation\n",
    "สัปดาห์นี้ มีการแนะนำการกระตุ้นแบบใหม่ ซึ่งเรียกว่า Rectified Linear Unit (ReLU)\"\n",
    "$$ a = max(0,z) \\quad\\quad\\text {# ReLU function} $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8297c6b9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e8631645d39248c7bba85e5608139136",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Canvas(toolbar=Toolbar(toolitems=[('Home', 'Reset original view', 'home', 'home'), ('Back', 'Back to previous …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt_act_trio()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "373ea1ac",
   "metadata": {},
   "source": [
    "<img align=\"right\" src=\"./images/C2_W2_ReLu_v1.png\"     style=\" width:380px; padding: 10px 20px; \" >\n",
    "การแปลเป็นภาษาไทย\n",
    "\"ตัวอย่างจากการบรรยายทางด้านขวาแสดงการประยุกต์ใช้ ReLU ในตัวอย่างนี้ คุณลักษณะ 'ความตระหนัก(awareness)' ที่ได้มาไม่ใช่แบบไบนารี แต่มีช่วงค่าต่อเนื่อง Sigmoid เหมาะที่สุดสำหรับสถานการณ์แบบเปิด/ปิดหรือแบบไบนารี ReLU ให้ความสัมพันธ์เชิงเส้นต่อเนื่อง นอกจากนี้ยังมีช่วง 'ปิด' ที่ผลลัพธ์เป็นศูนย์ คุณลักษณะ 'ปิด' ทำให้ ReLU เป็นการกระตุ้นแบบไม่เชิงเส้น ทำไมจึงจำเป็นต้องใช้? มาตรวจสอบด้านล่างนี้ "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f23d0683",
   "metadata": {},
   "source": [
    "### ทำไมต้องใช้ฟังก์ชันการกระตุ้นแบบไม่เชิงเส้น?  \n",
    "<img align=\"left\" src=\"./images/C2_W2_ReLU_Graph.png\"     style=\" width:250px; padding: 10px 20px; \" > ฟังก์ชันที่แสดงเป็นฟังก์ชันเชิงเส้นแบบทีละชิ้น (piecewise linear) ความชันคงที่ในช่วงเชิงเส้นและเปลี่ยนแปลงอย่างฉับพลันที่จุดเปลี่ยน ที่จุดเปลี่ยนจะมีการเพิ่มฟังก์ชันเชิงเส้นใหม่ ซึ่งเมื่อเพิ่มเข้ากับฟังก์ชันที่มีอยู่แล้ว จะทำให้เกิดความชันใหม่ ฟังก์ชันใหม่จะถูกเพิ่มที่จุดเปลี่ยน แต่จะไม่ส่งผลต่อผลลัพธ์ก่อนถึงจุดนั้น ฟังก์ชันการกระตุ้นแบบไม่เชิงเส้นเป็นผู้รับผิดชอบในการปิดการใช้งานอินพุตก่อนและบางครั้งหลังจากจุดเปลี่ยน แบบฝึกหัดต่อไปนี้จะให้ตัวอย่างที่จับต้องได้มากขึ้น"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f24a4e3e",
   "metadata": {},
   "source": [
    "แบบฝึกหัดนี้จะใช้เครือข่ายด้านล่างในปัญหาการถดถอยที่คุณต้องสร้างแบบจำลองเป้าหมายเชิงเส้นแบบแบ่งส่วน:\n",
    "<img align=\"center\" src=\"./images/C2_W2_ReLU_Network.png\"     style=\" width:650px; padding: 10px 20px; \">  \n",
    "เครือข่ายมี 3 หน่วยในเลเยอร์แรก แต่ละหน่วยจำเป็นต้องสร้างเป้าหมาย หน่วย 0 ถูกตั้งโปรแกรมไว้ล่วงหน้าและคงที่เพื่อจับคู่เซ็กเมนต์แรก คุณจะต้องปรับน้ำหนักและอคติในหน่วย 1 และ 2 เพื่อสร้างแบบจำลองเซ็กเมนต์ที่ 2 และ 3 หน่วยเอาต์พุตยังคงที่และเพียงแค่รวมผลลัพธ์ของเลเยอร์แรก\n",
    "\n",
    "โดยใช้แถบเลื่อนด้านล่าง ปรับน้ำหนักและอคติเพื่อให้ตรงกับเป้าหมาย เคล็ดลับ: เริ่มต้นด้วย `w1` และ `b1` และปล่อยให้ `w2` และ `b2` ป็นศูนย์จนกว่าคุณจะจับคู่เซ็กเมนต์ที่ 2 การคลิกแทนการเลื่อนจะเร็วกว่า หากคุณมีปัญหา อย่ากังวล ข้อความด้านล่างจะอธิบายรายละเอียดเพิ่มเติม"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "310ea5ef",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d89e83fedc19457bb6a7267d3ba2f530",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Canvas(toolbar=Toolbar(toolitems=[('Home', 'Reset original view', 'home', 'home'), ('Back', 'Back to previous …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "_ = plt_relu_ex()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2a364df",
   "metadata": {},
   "source": [
    "เป้าหมายของการฝึกนี้คือเพื่อชื่นชมว่าพฤติกรรมที่ไม่เป็นเชิงเส้นของ ReLU ให้ความสามารถที่จำเป็นในการปิดฟังก์ชันจนกว่าจะต้องการ มาดูกันว่ามันทำงานอย่างไรในตัวอย่างนี้\n",
    "<img align=\"right\" src=\"./images/C2_W2_ReLU_Plot.png\"     style=\" width:600px; padding: 10px 20px; \"> \n",
    "\"พล็อตทางด้านขวาประกอบด้วยผลลัพธ์ของหน่วยในเลเยอร์แรก เริ่มจากด้านบน หน่วย 0 มีหน้าที่รับผิดชอบส่วนแรกที่ทำเครื่องหมายด้วย 1 ทั้งฟังก์ชันเชิงเส้น $z$ และฟังก์ชันที่ตามหลัง ReLU$a$  จะแสดง คุณสามารถเห็นได้ว่า ReLU ตัดฟังก์ชันหลังช่วงเวลา [0,1] นี่เป็นสิ่งสำคัญเนื่องจากช่วยป้องกันไม่ให้หน่วย 0 แทรกแซงส่วนต่อไป\n",
    "\n",
    "หน่วย 1 มีหน้าที่รับผิดชอบส่วนที่ 2 ที่นี่ ReLU ทำให้หน่วยนี้เงียบจนกว่า x จะถึง 1 เนื่องจากหน่วยแรกไม่ได้มีส่วนร่วม ความชันสำหรับหน่วย 1, $w^{[1]}_1$, จึงเป็นเพียงความชันของเส้นเป้าหมาย ต้องปรับค่าไบแอสเพื่อให้ผลลัพธ์เป็นลบจนกว่า x จะถึง 1 โปรดสังเกตว่าการมีส่วนร่วมของหน่วย 1 ขยายไปถึงส่วนที่ 3 ด้วย\n",
    "\n",
    "หน่วย 2 มีหน้าที่รับผิดชอบส่วนที่ 3 ReLU ทำให้ผลลัพธ์เป็นศูนย์อีกครั้งจนกว่า x จะถึงค่าที่ถูกต้อง ความชันของหน่วย $w^{[1]}_2$, ต้องตั้งค่าเพื่อให้ผลรวมของหน่วย 1 และ 2 มีความชันที่ต้องการ ค่าไบแอสจะถูกปรับอีกครั้งเพื่อให้ผลลัพธ์เป็นลบจนกว่า x จะถึง 2\n",
    "\n",
    "คุณสมบัติ \"ปิด\" หรือ \"ปิดใช้งาน\" ของฟังก์ชันการกระตุ้น ReLU ช่วยให้โมเดลสามารถเย็บส่วนเชิงเส้นเข้าด้วยกันเพื่อสร้างฟังก์ชันที่ไม่เชิงเส้นที่ซับซ้อน\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2e044b8",
   "metadata": {},
   "source": [
    "## ขอแสดงความยินดี!\n",
    "คุณคุ้นเคยกับ ReLU และความสำคัญของพฤติกรรมแบบไม่เชิงเส้นมากขึ้นแล้ว"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
